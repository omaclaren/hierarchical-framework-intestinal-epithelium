{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Simulation functions  \n",
    "OJM  \n",
    "Similarly to data analysis functions, separate out here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "from scipy import stats\n",
    "import scipy.interpolate as interpolate\n",
    "import scipy.special as special\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import errno\n",
    "%matplotlib inline\n",
    "from clawpack import pyclaw\n",
    "from clawpack import riemann\n",
    "#%load_ext autoreload #are these needed in notebook??\n",
    "#%autoreload 2 #are these needed in notebook??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%run data_analysis_functions.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####proliferation and velocity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def proliferation_profile(p=np.array([1.0,2.0,0.001,0.001]),x_max=1.):\n",
    "    \"\"\"\n",
    "    simple piecewise constant proliferation profile function\n",
    "    k(x)  = p_i, x_0<x<x_i for x in [0,1]\n",
    "    -\n",
    "    notes:\n",
    "    p_i includes the very right-most point - only used at edge of domain. Hence -1.0 in indexing.\n",
    "    evaluate at multiple points, if necessary, using list comprehensions, e.g. [k(x_i) for x_i in x]\n",
    "    -\n",
    "    todos:\n",
    "    constrain so non-negative near base?\n",
    "    \"\"\"\n",
    "    \n",
    "    k_prolif= lambda x: p[np.floor((x/x_max)*(p.size-1.0))]\n",
    "    \n",
    "    #p_grid= np.linspace(0.,1.0,p.size+1)\n",
    "    #k_prolif=lambda xi: [p[i] for i in range(0,p_grid.size-1) if (xi >= x_max*p_grid[i]) & (xi <= x_max*p_grid[i+1])][0]\n",
    "    #p=np.array([1.0,2.0,0.0,0.5])\n",
    "    #grid= np.linspace(0.,1.0,p.size+1)\n",
    "    #print p\n",
    "    #print grid\n",
    "    #k_prolif= lambda xi: np.piecewise(xi,[(xi >= grid[i]) & (xi <= grid[i+1]) for i in range(len(grid)-1)],p)\n",
    "    #grid= np.linspace(0.,1.0,p.size+1)\n",
    "    \n",
    "    #vector evaluation?\n",
    "    #k_prolif= lambda x_i: np.piecewise(x_i,[(x_i >= x_max*p_grid[i]) & (x_i <= x_max*p_grid[i+1]) for i in range(len(p_grid)-1)],p)\n",
    "    \n",
    "    return k_prolif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def velocity_from_integrated_proliferation(k_prolif,x_lower=0.0):\n",
    "    \"\"\"\n",
    "    velocity as defined by integral of proliferation function\n",
    "    -\n",
    "    notes:\n",
    "    could be given directly then differentiated? what does this do? any bias?\n",
    "    in many cases could be integrated analytically\n",
    "    is this numerical scheme any good?\n",
    "    currently makes no use of previous integrations - could store results/give recursively?\n",
    "    argument for using e.g. splines/gps? - can integrate once\n",
    "    \"\"\"\n",
    "    #could be much more efficient! can express analytically duh!\n",
    "    return lambda x_i: sp.integrate.quad(k_prolif,x_lower,x_i)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####pyclaw functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def setup(nx=100, kernel_language='Python', use_petsc=False, solver_type='classic', weno_order=5, \n",
    "    time_integrator='SSP104', outdir_claw='./_output',initial_profile_f = lambda x: x, \n",
    "    velocity_f= lambda x: x,norm_out_times= np.linspace(0.0,1.0,10),domain_length=100.):\n",
    "\n",
    "    \"\"\"\n",
    "    based on example ??? from pyclaw. updated to use color equation riemann solver.\n",
    "    sets up a pyclaw controller.\n",
    "    -\n",
    "    options:\n",
    "    normalised time?\n",
    "    un-normalised space!\n",
    "    -\n",
    "    todos:\n",
    "    write own riemann solver\n",
    "    kernal language?\n",
    "    re-check boundary conditions?\n",
    "    ?\n",
    "    \"\"\"\n",
    "    \n",
    "    #--solver options--\n",
    "    if use_petsc:\n",
    "        import clawpack.petclaw as pyclaw\n",
    "    else:\n",
    "        from clawpack import pyclaw\n",
    "\n",
    "    #NEEDS TO BE SET TO PYTHON! Go over.\n",
    "    if kernel_language == 'Fortran':\n",
    "        #riemann_solver = riemann.advection_1D #OLD SOLVER\n",
    "        riemann_solver = riemann.vc_advection_1D\n",
    "    elif kernel_language == 'Python':\n",
    "        #riemann_solver = riemann.advection_1D_py.advection_1D #OLD SOLVER\n",
    "        riemann_solver = riemann.vc_advection_1D_py.vc_advection_1D\n",
    "\n",
    "    if solver_type=='classic':\n",
    "        solver = pyclaw.ClawSolver1D(riemann_solver)\n",
    "    elif solver_type=='sharpclaw':\n",
    "        solver = pyclaw.SharpClawSolver1D(riemann_solver)\n",
    "        solver.weno_order = weno_order\n",
    "        solver.time_integrator = time_integrator\n",
    "    else: raise Exception('Unrecognized value of solver_type.')\n",
    "\n",
    "    solver.kernel_language = kernel_language #NEEDS TO BE SET TO PYTHON\n",
    "    #--\n",
    "    \n",
    "    #--boundary conditions--\n",
    "    #label\n",
    "    solver.bc_lower[0] = pyclaw.BC.extrap\n",
    "    solver.bc_upper[0] = pyclaw.BC.extrap\n",
    "    #velocity\n",
    "    solver.aux_bc_lower[0] = pyclaw.BC.wall\n",
    "    solver.aux_bc_upper[0] = pyclaw.BC.extrap\n",
    "\n",
    "    #--domain and state--\n",
    "    x = pyclaw.Dimension('x',0.0,domain_length,nx)\n",
    "    domain = pyclaw.Domain(x)\n",
    "    num_aux= 1 #velocity\n",
    "    state = pyclaw.State(domain,solver.num_eqn,num_aux)\n",
    "\n",
    "    #--initial data & time-independent aux--\n",
    "    #TODO FUNCTION EVALUATIONS AND LIST COMP.\n",
    "    x_centres = state.grid.x.centers #CAREFUL OF CELL CENTRES?? TODO\n",
    "    state.q[0,:] = [initial_profile_f(xc) for xc in x_centres]\n",
    "    state.aux[0,:]= [velocity_f(xc) for xc in x_centres]\n",
    "    \n",
    "    #--controller and solver--\n",
    "    claw = pyclaw.Controller()\n",
    "    claw.keep_copy = True\n",
    "    claw.solution = pyclaw.Solution(state,domain)\n",
    "    claw.solver = solver\n",
    "\n",
    "    if outdir_claw is not None:\n",
    "        claw.outdir = outdir_claw\n",
    "    else:\n",
    "        claw.output_format = None\n",
    "    \n",
    "    claw.t0 = 0.0 #normed time?\n",
    "    claw.tfinal =1.0 #normed time?\n",
    "    claw.output_style = 2 #required for out_times? see help.\n",
    "    claw.out_times= norm_out_times\n",
    "    \n",
    "    #import logging\n",
    "    #solver.logger.setLevel(logging.CRITICAL)\n",
    "    claw.verbosity = 0\n",
    "    \n",
    "    #import logging\n",
    "    #logger = logging.getLogger('claw')\n",
    "    #logger.setLevel(logging.CRITICAL)\n",
    "    #claw.setplot = setplot #required?\n",
    "\n",
    "    return claw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def setplot(plotdata):\n",
    "    \"\"\" \n",
    "    Plot solution using VisClaw.\n",
    "    \"\"\" \n",
    "    plotdata.clearfigures()  # clear any old figures,axes,items data\n",
    "\n",
    "    plotfigure = plotdata.new_plotfigure(name='q', figno=1)\n",
    "\n",
    "    # Set up for axes in this figure:\n",
    "    plotaxes = plotfigure.new_plotaxes()\n",
    "    plotaxes.ylimits = [-.2,1.0]\n",
    "    plotaxes.title = 'q'\n",
    "\n",
    "    # Set up for item on these axes:\n",
    "    plotitem = plotaxes.new_plotitem(plot_type='1d_plot')\n",
    "    plotitem.plot_var = 0\n",
    "    plotitem.plotstyle = '-'\n",
    "    plotitem.color = 'b'\n",
    "    plotitem.kwargs = {'linewidth':2,'markersize':5}\n",
    "\n",
    "    return plotdata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Single sim and multiple/optimisation sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def construct_and_run_sim(data_dir='../data-working/TXT_BrdU/',sample_type='BrdU',\n",
    "                          actual_out_times= np.array([60.,120.,360.,600.,1080.,1920.]),precision_time=4,\n",
    "                          p=np.array([1.0,2.0,0.01,0.01,0.01]),x_min=0.,x_max=100.,nx=100):\n",
    "    \"\"\"\n",
    "    takes a directory, set of times and \n",
    "    finds start file corresponding to first time in set\n",
    "    gets initial profile fit\n",
    "    uses as IC\n",
    "    solves\n",
    "    returns simulation results at desired times in np arrays\n",
    "    \"\"\"\n",
    "    #get initial condition\n",
    "    files_in_dir= os.listdir(data_dir)\n",
    "    files_in_dir.sort() #assumes files have same name format!!\n",
    "\n",
    "    start= actual_out_times[0]\n",
    "    time_format= '%0'+('%1d' % precision_time)+'d'\n",
    "    file_ic= get_data_file(data_dir,time_format%start)\n",
    "    \n",
    "    density_results= process_and_fit_label_data_file(data_dir=data_dir,file_to_fit=file_ic,sample_type=sample_type,x_max=x_max,do_plot=False)\n",
    "    initial_profile_f= density_results[-1]\n",
    "    \n",
    "    velocity_f= velocity_from_integrated_proliferation(proliferation_profile(p=p,x_max=x_max))\n",
    "    \n",
    "    #convert between experimental times and simulation times \n",
    "    norm_out_times= (actual_out_times-min(actual_out_times))/(max(actual_out_times)-min(actual_out_times))\n",
    "\n",
    "    #set up and run simulation \n",
    "    controller= setup(nx=nx,initial_profile_f=initial_profile_f,velocity_f=velocity_f,norm_out_times=norm_out_times)\n",
    "    #controller.verbosity= 0\n",
    "    controller.run()\n",
    "\n",
    "    #extract (all) simulation results\n",
    "    output_shape= [np.size(controller.frames[0].state.q[0,:],axis=0),np.size(controller.frames,axis=0)]\n",
    "    #print output_shape\n",
    "    labels= np.zeros(output_shape)\n",
    "    x_centres= np.zeros(output_shape)\n",
    "    velocity= np.zeros(output_shape)\n",
    "    for i in range(0,np.size(controller.out_times,axis=0)):\n",
    "        labels[:,i]= controller.frames[i].state.q[0,:]\n",
    "        #don't actually vary with time!\n",
    "        x_centres[:,i]= controller.frames[0].state.grid.c_centers[0]\n",
    "        velocity[:,i]= controller.frames[0].state.aux[0,:]\n",
    "\n",
    "    return labels, velocity, x_centres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def opt_sim(data_dir='../data-working/TXT_BrdU/',sample_type='BrdU',\n",
    "                          actual_out_times= np.array([60.,120.,360.,600.,1080.,1920.]),times_to_fit_i=[5],precision_time=4,\n",
    "                          p0=np.array([1.0,2.0,0.01,0.01,0.01]),p_var_i=[],reg_param= 0.1,penalty_order=1,x_min=0.,x_max=100.,k=3,s=15,nx=100):\n",
    "    \"\"\"\n",
    "    optimisation solution to inverse problem\n",
    "    -\n",
    "    notes\n",
    "    a key difficulty is choosing proper comparison grid\n",
    "    also, systematic regularisation - dependence on number of parameters etc? determining reg. parameter\n",
    "    -\n",
    "    structure\n",
    "    initial condition\n",
    "    data for fitting\n",
    "    model definition\n",
    "    residuals function\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    #---\n",
    "    #get initial condition\n",
    "    files_in_dir= os.listdir(data_dir)\n",
    "    files_in_dir.sort() #assumes files have same name format!!\n",
    "\n",
    "    start= actual_out_times[0]\n",
    "    time_format= '%0'+('%1d' % precision_time)+'d'\n",
    "    file_ic= get_data_file(data_dir,time_format%start)\n",
    "    \n",
    "    density_results= process_and_fit_label_data_file(data_dir=data_dir,file_to_fit=file_ic,\n",
    "                                                     sample_type=sample_type,x_max=x_max,\n",
    "                                                     do_plot=False)\n",
    "    initial_profile_f= density_results[-1]\n",
    "    \n",
    "    #---\n",
    "    #data for comparison. NOTE - get \n",
    "    #TODO - use times_to_fit_i?\n",
    "    x_data_to_fit= np.tile(np.arange(x_min,x_max),(actual_out_times.size-1,1))\n",
    "    x_data_to_fit= np.transpose(x_data_to_fit)\n",
    "    label_data_at_x_data= np.zeros(x_data_to_fit.shape)\n",
    "\n",
    "    for i in range(0,actual_out_times.size-1):\n",
    "        current_time= actual_out_times[i+1]\n",
    "        file_current= get_data_file(data_dir,time_format%current_time)\n",
    "        print file_current\n",
    "        data_result= process_and_fit_label_data_file(data_dir=data_dir,file_to_fit=file_current,sample_type=sample_type,k=k,s=s,x_max=100,do_plot=False)\n",
    "        #careful here - data grid concept needs to be tidied up.\n",
    "        label_data_at_x_data[:,i]= np.append(data_result[0],np.zeros(x_max-data_result[0].size))\n",
    "    \n",
    "    #convert between experimental times and simulation times \n",
    "    norm_out_times= (actual_out_times-min(actual_out_times))/(max(actual_out_times)-min(actual_out_times))\n",
    "    \n",
    "    #---\n",
    "    #function for one sim.\n",
    "    def model(p_var):\n",
    "        \"\"\"\n",
    "        simulation model\n",
    "        -\n",
    "        notes:\n",
    "        output formats for each quantity are -\n",
    "        [column of results at time 1|column of results at time 2|...etc...]\n",
    "        uses arguments from outer function - bad practice?\n",
    "        bit of a hack with 'global' vs. local arguments.\n",
    "        \"\"\"\n",
    "        #HACK!\n",
    "        #try_count= 0\n",
    "        #while try_count<5:\n",
    "        #    try:\n",
    "                #code with possible error\n",
    "                #set up and run simulation \n",
    "        #        velocity_f= velocity_from_integrated_proliferation(proliferation_profile(p=p,x_max=x_max))\n",
    "        #        controller= setup(nx=nx,initial_profile_f=initial_profile_f,velocity_f=velocity_f,norm_out_times=norm_out_times)\n",
    "        #        #controller.verbosity= 0\n",
    "        #        controller.run()\n",
    "        #    except:\n",
    "        #        print 'adding noise/making positive to try avoid numerical difficulties'\n",
    "        #        print p\n",
    "                #print p_current\n",
    "                #p= p+np.random.normal(0,0.1,p.size)\n",
    "        #        p= np.abs(p+np.random.normal(0,0.1,p.size))\n",
    "        #        try_count= try_count+1\n",
    "        #        continue\n",
    "        #    else:\n",
    "        #         #the rest of the code\n",
    "        #         break\n",
    "        p= p0\n",
    "        p[p_var_i]= p_var[p_var_i]\n",
    "        #print p[p_fixed_i]== p0[p_fixed_i]\n",
    "        #print 'here'\n",
    "        velocity_f= velocity_from_integrated_proliferation(proliferation_profile(p=p,x_max=x_max))\n",
    "        controller= setup(nx=nx,initial_profile_f=initial_profile_f,velocity_f=velocity_f,norm_out_times=norm_out_times)\n",
    "        #controller.verbosity= 0\n",
    "        controller.run()\n",
    "\n",
    "        #extract (all) simulation results\n",
    "        output_shape= [np.size(controller.frames[0].state.q[0,:],axis=0),np.size(controller.frames,axis=0)]\n",
    "        #print output_shape\n",
    "        labels= np.zeros(output_shape)\n",
    "        x_centres= np.zeros(output_shape)\n",
    "        velocity= np.zeros(output_shape)\n",
    "        for i in range(0,np.size(controller.out_times,axis=0)):\n",
    "            labels[:,i]= controller.frames[i].state.q[0,:]\n",
    "            #don't actually vary with time!\n",
    "            x_centres[:,i]= controller.frames[0].state.grid.c_centers[0]\n",
    "            velocity[:,i]= controller.frames[0].state.aux[0,:]\n",
    "\n",
    "        return labels, velocity, x_centres\n",
    "\n",
    "    #---\n",
    "    #residuals function\n",
    "    def residuals(p_var_current,flatten_residual=True):#,p0=np.array([1.0,2.0,0.01,0.01,0.01]))#,p_fixed_i=[]):#,x_data_to_fit=x_data_to_fit,label_data_to_fit=label_data_to_fit,times_to_fit=[]):\n",
    "        \"\"\"\n",
    "        residuals between model solutions and data\n",
    "        -\n",
    "        notes\n",
    "        data and model results are matrices/arrays!\n",
    "        in 'column vector' storage format?\n",
    "        -\n",
    "        Plan\n",
    "        general outline\n",
    "        -at a given time\n",
    "        [vectorised]\n",
    "        --at a set of data comparison x points\n",
    "        ---get data values\n",
    "        ---get solution values (via interp.)\n",
    "        ---compute residual and store as column vector in residual matrix\n",
    "        -(in another function) square values and sum to get a single scalar\n",
    "        approach\n",
    "        -test cell to consider a vector of data values and test sim and calc residual\n",
    "        \"\"\"\n",
    "        #get solutions at all times > t0. #use e.g. structured arrays??\n",
    "        results= model(p_var=p_var_current)\n",
    "        labels_model= results[0][:,1:]\n",
    "        x_centres_model= results[2][:,1:]\n",
    "\n",
    "        #data grid. TODO - do better. Note: don't include initial condition so one index smaller.\n",
    "        #use e.g. structured arrays?? For now - collect all but only compare subset. Inefficient.\n",
    "        label_model_at_x_data_current= np.zeros(x_data_to_fit.shape[0])\n",
    "        residual_at_x_data= np.zeros(x_data_to_fit.shape)\n",
    "        #times_to_fit_i\n",
    "        for i in np.subtract(times_to_fit_i,1):\n",
    "            #TODO - assert i>0\n",
    "            \n",
    "            #current_time= actual_out_times[i+1]\n",
    "            #file_current= get_data_file(data_dir,time_format%current_time)\n",
    "            #print file_current\n",
    "            #data_result= process_and_fit_label_data_file(data_dir=data_dir,file_to_fit=file_current,sample_type=sample_type,k=k,s=s,x_max=100,do_plot=False)\n",
    "            #careful here - data grid concept needs to be tidied up.\n",
    "            #label_data_at_x_data= np.append(data_result[0],np.zeros(x_max-data_result[0].size))\n",
    "            label_model_at_x_data_current= np.interp(x_data_to_fit[:,i],x_centres_model[:,i],labels_model[:,i])\n",
    "            residual_at_x_data[:,i]= label_data_at_x_data[:,i]-label_model_at_x_data_current\n",
    "            #temp\n",
    "            #plt.plot(residual_at_x_data[:,i])\n",
    "\n",
    "        #temp\n",
    "        #plt.show()\n",
    "        if flatten_residual:\n",
    "            return np.ravel(residual_at_x_data) #ravel flattens into single vector\n",
    "        else:\n",
    "            return residual_at_x_data\n",
    "    #---\n",
    "    #Optimisation\n",
    "    #currently only one step.\n",
    "    import scipy.optimize as sciopt\n",
    "    flatten_residual= True\n",
    "    \n",
    "    #plsq= sciopt.leastsq(residuals,p0,args=(flatten_residual))\n",
    "    #pest= plsq[0]\n",
    "    #bounds= [(-10,10) for i in range(p.size)]\n",
    "    #max_iter= 10\n",
    "    #max_fev= 20\n",
    "    #note - use centered parameters in penalty. CAREFUL OF FIXED PARAMETERS!!!\n",
    "    p_var0= p0[p_var_i]\n",
    "    sum_square_penalised= lambda p_var: (1/np.float(len(times_to_fit_i)))*np.linalg.norm(residuals(p_var_current=p_var))\\\n",
    "        +reg_param*(np.linalg.norm(p_var-np.mean(p_var),ord=penalty_order))\n",
    "    #out= sciopt.minimize(sum_sq,p0,bounds=bounds,method=),method='Nelder-Mead'\n",
    "    #out= sciopt.minimize(sum_square_penalised,p0,method='Nelder-Mead',options=dict({'maxiter':10,'maxfev':20}))\n",
    "    out= sciopt.minimize(sum_square_penalised,p_var0,method='Nelder-Mead')\n",
    "    #pest= out.x\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stochastic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
